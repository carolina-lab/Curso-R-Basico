---
title: "Sesion 4"
author: "Laboratorio Fintrade"
date: "16/7/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Contenido

- Regresion lineal simple
    + Ejemplo 1
    + Ejemplo 2
- Regresion lineal multiple
    + Ejemplo 1
    + Supuestos del modelo
        - Linealidad
        - Autocorrelacion
        - Homoscedasticidad
        - Normalidad
        - Multicolinealidad
- Ejercicio final



## Regresiones lineales simples

Las regresiones lineales comunmente son usadas para variables dependientes continuas. Aquí la función es lineal, es decir, requiere la determinación de dos parámetros: la pendiente y la ordenada en el origen.

## Aplicación en RStudio

La base de datos con la que se trabajará será "Boston"



```{r ins}
library(MASS)
data("Boston")
attach(Boston)
```


Con esta regresión se pretende predecir el valor de la vivienda en función de la tasa impositiva de las propiedades en Boston. Para esto, se emplea la función lm() para generar un modelo lineal.

## Hipótesis

- Hipótesis nula: el valor de la vivienda de las casas ocupadas NO provoca una subida en la tasa de impuesto en las propiedades


- Hipótesis alternativa: el valor de la vivienda de las casa ocupadas prova una subida en la tasa de impuesto en las propiedades



```{r mod}
reg1<-lm(medv~tax)
```

La función lm() genera un objeto que almacena toda la información del modelo, y para visualizar los principales parámetros del modelo generado se utiliza summary().

```{r sum}
summary(reg1)
```


Se observa que la variable "tax" está significativamente relacionada con la variable respuesta. La evaluación del modelo en conjunto puede hacerse a partir de los valores RSE o del valor $R^2$ arrojado por el summary.

- Residual standar error (RSE): En promedio, cualquier predicción del modelo se aleja 8.133 unidades del verdadero valor. 
- $R^2$: El predictor **tax**  empleado en el modelo es capaz de explicar el 54.44% de la variabilidad observada en el precio de las viviendas.



Los dos coeficientes de regresión ($β0$ y $β1$) estimados por el modelo son significativos y se pueden interpretar como:

- Intercept($β0$): El valor promedio del precio de la vivienda cuando el tax es 0, es de 32.97 unidades.
- Predictor tax ($β1$): por cada unidad que se incrementa el predictor tax el precio de la vivienda disminuye en promedio 0.025 unidades.

La estimación de todo coeficiente de regresión tiene asociada un error estándar, por lo tanto todo coeficiente de regresión tiene su correspondiente intervalo de confianza.

```{r con}
confint(reg1, level = 0.95)
```

Dado que el p-value del predictor **tax** ha resultado significativo a un 0.05%, su intervalo de confianza del 95% no contiene el valor de 0.

Por otro lado, la creación de un modelo de regresión lineal simple suele acompañarse de una representación gráfica superponiendo las observaciones con el modelo. Además de ayudar a la interpretación, es el primer paso para identificar posibles violaciones de las condiciones de la regresión lineal.


```{r at}
attach(Boston)
plot(x = lstat, y = medv, main = "medv vs tax", pch = 20, col = "grey30")
abline(reg1, lwd = 3, col = "red")
```


La representación gráfica de las observaciones muestra que la relación entre ambas variables estudiades es del todo linea.

Una de las mejores formas de confirmar que las condiciones necesarias para un modelo de regresión lineal simple por mínimos cuadrados se cumplen es mediante el estudio de los residuos del modelo.

En R, los residuos se almacenan dentro del modelo bajo el nombre de residuals. R genera automáticamente los gráficos más típicos para la evaluación de los residuos de un modelo.


```{r }
par(mfrow = c(2,2))
plot(reg1)
```

Los residuos confirman que los datos  se distribuyen de forma lineal,  (plot1).Sin embargo, se observa que la distribución de los residuos no es normal (plot2). 

## Ahora probemos con otro modelo

```{r r2}
attach(Boston)
reg2<-lm(medv~lstat)
summary(reg2)
```

## Gráficamente


```{r }
par(mfrow = c(2,2))
plot(reg2)
```



# REGRESION LINEAL MULTIPLE

La regresión lineal múltiple permite generar un modelo lineal en el que el valor de la variable dependiente o respuesta (Y) se determina a partir de un conjunto de variables independientes llamadas predictores (X1, X2, X3…). Es una extensión de la regresión lineal simple, por lo que es fundamental comprender esta última. Los modelos de regresión múltiple pueden emplearse para predecir el valor de la variable dependiente o para evaluar la influencia que tienen los predictores sobre ella (esto último se debe que analizar con cautela para no malinterpretar causa-efecto).

Los modelos lineales múltiples siguen la siguiente ecuación:

Yi=(β0+β1X1i+β2X2i+⋯+βnXni)+ei

- β0 : es la ordenada en el origen, el valor de la variable dependiente Y cuando todos los predictores son cero.

- βi: es el efecto promedio que tiene el incremento en una unidad de la variable predictora Xi sobre la variable dependiente Y, manteniéndose constantes el resto de variables. Se conocen como coeficientes parciales de regresión.

- ei: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo.

Es importante tener en cuenta que la magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.

## modelo

```{r importar base, message=FALSE, warning=FALSE, include=FALSE}
library(readxl)

library(httr)# Otra forma de encontrar direcciones 
library(psych)
library(dplyr)
pib<-'https://raw.githubusercontent.com/Fintrade2020/Curso-R-Basico/master/Bases/PIB.xlsx'
GET(pib, write_disk(PIB <- tempfile(fileext = ".xlsx"))) 
PIB <- read_excel(PIB)
View(PIB)
```

```{r DEFI,include=FALSE}
pib=PIB$PIB
ocupados=PIB$Ocupados
formacion=PIB$FBCF
```


```{r modelo}
modelo1 <- lm(pib ~ ocupados + formacion, data = PIB )
summary(modelo1)
```

#### Conceptos basicos de la salida de regresion

* Residuals: son las diferencias entre las respuestas observadas en cada combinación de valores de las variables regresoras y la correspondiente predicción de la respuesta calculada.

* Estimate: corresponde a las estimaciones de los coeficientes del modelo. Los coeficientes de las variables regresoras siempre tienen interpretación.

* Std Error: representa el error estándar de cada coeficiente del modelo.

* t value: es el valor que toma el estadístico de prueba.

* Pr(>|t|): corresponde al valor-p para las pruebas de hipótesis de que los coeficientes son ceros. Si esta valor-p es menor que el nivel de significancia, que por defecto es del 5%, entonces se rechaza la hipótesis nula y por lo tanto, existe significancia en la regresión. A menudo, la prueba de hipótesis de que el intecerpto es cero es poco útil, ya que éste depende de la naturaleza del problema. En cambio, las pruebas de hipótesis de que los coeficientes de las variables regresoras son cero, son muy importantes; debido a que, si se prueba que el coeficiente es cero, entonces la variable regresora no tendría ningún aporte al modelo.

* Signif codes: representa el nivel de significancia de las variables regresoras para la predicción del modelo de regresión. Entre más estrellas se le asigne a la regresora, más contribuye con el modelo.

* Residual standard error: es la estimación de la desviación estandar. Este valor da una idea de cuán lejos están los valores del modelo ajustado a los valores observados de la variable respuesta.

* Multiple R-squared: este valor expresa la proporción de la variación explicada por el modelo; es decir, por las variables explicativas. 

* F-statistic: equivale al estadístico F. Éste permite constrastar la hipótesis nula de que el valor poblacional R es cero, lo cual, equivale a contrastar la hipótesis de que los coeficientes de las variables regresoras son cero.

* p-value: indica la importancia del modelo mediante una prueba de hipótesis donde la hipótesis nula es que todos los coeficientes del modelo son cero. 



Con base a lo anterior, el modelo con todas las variables introducidas como predictores tiene un R2 alta (0.977), es capaz de explicar el 97,7% de la variabilidad observada en la esperanza de vida. El p-value del modelo es significativo (2.2e-16) por lo que se puede aceptar que el modelo no es por azar, al menos uno de los coeficientes parciales de regresión es distinto de 0. Logrando observar que todos los coeficientes son significativos y contribuyen al modelo.





## Supuestos del modelo

#### Supuesto de linealidad

Existe linealidad si se presenta una relación significativa entre la variable que se quiere predecir y las otras variables. Puede usarse el coeficiente "R cuadrado ajustado", para saber si existe linealidad (mayor o igual a 0.7 suele ser "indicio" de linealidad).




#### Supuesto de Autocorrelacion

Este supuesto asume que los residuos no están auto-correlacionados, por lo cual son independientes. La autocorrelacion es cuando el residuo en la predicción de un valor es afectado por el residuo en la predicción del valor más cercano. Esta autocorrelacion suele presentarse en series de tiempo.

Para validar la independencia de los residuos puede usarse el test durbin-watson, cuyo resultado estará cerca de 2 cuando los residuos son independientes. Si el valor está entre 1.5 y 2.5 puede concluirse que no existe dependencia de los residuos.

```{r autocorr}
library(car)
dwt(modelo1)
```

De acuerdo con la prueba Durbin Watson, un valor cercano a 2, indica que no hay autocorrelación, > 1 correlaciones positivas; < 1 correlaciones negativas,si esta entre como 1.8 y 2.2 y el valor es menor a 0.01 rechazo la hipotesis nula de que no hay autocorrelacion.




#### Supuesto de homoscedasticidad

Este supuesto asume que los residuos en las predicciones son constantes en cada predicción (es decir, varianza constante). Este supuesto valida que los residuos no aumenta ni disminuye cuando se predicen valores cada vez más altos o mas pequeños. A esta constancia en los errores de predicción le dicen "homocedasticidad", y cuando los errores varían, le dicen "heterocedasticidad".

##### Test de Breush - Pagan
```{r homos}
library(lmtest)
bptest(modelo1)
```

#### Supuesto de normalidad de residuos

Este supuesto asume que los residuos deben presentar una distribución normal, y la ausencia de normalidad supone  poca precisión en los intervalos de confianza creados por el modelo.


##### Test Jarque Bera 

Este test indica que tanto se desvían los indicadores de kurtosis y asimetría de los normales.

```{r norm}
library(tseries)
jarque.bera.test(x = modelo1$residuals)
```

#### Supuesto de multicolinealidad

La multicolinealidad es la relación de dependencia lineal fuerte entre más de dos variables explicativas en una regresión múltiple que incumple el supuesto de Gauss-Markov cuando es exacta.

En otras palabras, la multicolinealidad es la correlación alta entre más de dos variables explicativas.


```{r multicol}
library(car)
vif(modelo1)
```
Si el VIF es mayor o igual a 10, hay problemas de multicolinealidad

## Ejercicio propuesto

Instalar el paquete **dplyr**



El objetivo es que repliquen el código que se hará a continuación, analicen las salidas del modelo e identifiquen las variables más significativas y si el modelo es adecuado para predecir. Se utilizará la librería **trees** esta contiene la cicunferencia, la altura y el volúmen del tronco de árboles de cerezos

```{r ejercicio final}
library(dplyr)
data("trees")
head(trees)
View(trees)
```

- 1. Deben hacer un diagrama de dispersión usando la librería **psych** de este modo:

```{r ejercicio final2}
#library(psych)
#pairs.panels(##nombre de la base de datos)
```


- Realizar histogramas para las variables Girth, Height, Volume
- Realizar un boxplot o diagrama de cagas para las variables Girth, Height, Volume
- Construir un modelo de regresión donde la variable dependiente sea Volume y las independientes Height y Girth, previamente identificar cuále es la hipótesis nula y alternativa con la ayuda del buscador de la consola, es decir, **??trees**
- Identificar los estadísticos del modelo con la función **summary** (residual standar error, multiple r-squared, significancia global y significancia de las variables)
- Hacer un análisis estadístico de las variables
- Replicar la validación de los supuestos de normalidad, homocedasticidad y autocorrelación para el modelo.
