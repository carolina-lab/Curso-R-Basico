---
title: "Regresión lineal simple"
author: "Laboratorio FINTRADE"

 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Estadística descriptiva

#Medidas de tendencia central
Las variables continuas se pueden resumir con un úmero que represente la “mitad” del conjunto de números conocidos como estadístico de tendencia central.

#Media aritmética
Es la suma de los valores dividido por el número de observaciones. La función de “R” para la media aritmética es mean(variable).Por ejemplo: 

```{r c}
x<-seq(1,100,2)
x

mean(x)
```
##Media recortada:
Se recorta una proporción de observaciones por ambos lados de la distribución. La función de “R” para la media recortada es mean(“variable”, trim=0,2). Por ejemplo:


```{r m}
y<-c(2,4,5,6,7,2,5,4,2,9,2,2,3,5,8,7)
y

mean(y)
mean(y, trim=0.2)
```

##Mediana
Es el valor de la variable que deja el mismo número de datos antes y después de él, una vez ordenados estos, cuando las observaciones se ordenan. La función de “R” para la mediana es median(“variable”)


```{r med}
median(x)
```

##Moda:
Es el valor con una mayor frecuencia en una distribución de datos. Para calcular la moda en “R” utilizamos la función table(variable) que nos dice cuántas veces aparece cada número dentro de un vector, por lo que la moda será el número que más veces se repita.



```{r med}
table(x)

table(y)

which.max(table(x))


which.max(table(y))
```
##Medidas de posición

## Cuartiles
Son los tres valores de la variable que dividen a un conjunto de datos ordenados en cuatro partes iguales. La función de “R” para los cuartiles son quantile(variable)


```{r q}
quantile(x)
summary(x)
```

##Percentiles
Son los 99 valores que dividen la serie de datos en 100 partes iguales.
```{r pe}
quantile(x,0.1)

quantile(x,0.15)

quantile(x,0.45)
```
## Medidas de dispersión

##Varianza:
Es la esperanza del cuadrado de la desviación típica de dicha variable respecto a su media. La función de “R” para la varianza es var(variable)

```{r var}
var(x)
```

## Desviación estándar
Es la raíz cuadrada de la varianza. La función de “R” para la desviación estándar es sd(variable)

```{r sd}
sd(x)
```

## Ejemplo

Se utilizará la base de datos "BostonHousing" que contiene el paquete mlbench y usando la función **tapply** y calcular, la media, la median, media recortada, los cuartiles, desviación estádar, de las concentraciones de óxidos de nitrógeno en los ríos 


```{r winw}
library(mlbench)
data("BostonHousing")
View(BostonHousing)
head(BostonHousing)
```

Como se pide calcular medidas de tendencia central, de posición y dispersión solo del número de muertes, utilizamos la función tapply, que nos calcula todos esos parámetros 

# Para la media
```{r wmed}

media<-tapply(BostonHousing$nox, INDEX = BostonHousing$chas, FUN = mean)
media

```

##Para la mediana
```{r bhme}
mediana<-tapply(BostonHousing$nox, INDEX = BostonHousing$chas, FUN = median)
mediana
```

## Para la media recortada

```{r rec}

m.recort<-tapply(BostonHousing$nox, INDEX = BostonHousing$chas, mean,trim=0.2)
m.recort
```
## Para los cuartiles



```{r cuart}
cuartiles<-tapply(BostonHousing$nox, INDEX = BostonHousing$chas, FUN = quantile)
cuartiles
```


## Desviación estándar
```{r winw}
desviación.estándar<-tapply(BostonHousing$nox, INDEX = BostonHousing$chas, FUN = sd)
desviación.estándar
```
## Contraste de hipótesis

Los contrastes de hipótesis sirven para tomar decisiones acerca de las características poblacionales

- Una hipótesis estadística es cualquier afirmación, verdadera o falsa sobre alguna característica desconocida de las poblaciones

#Ejemplo
- La hipótesis nula sería que la desintegración familiar de los padres NO  provoca baja autoestima en los hijos. Se deriva directamente de la declaración del problema y es denotada como HO
- La hipótesis alternativa sería que la desintegración familiar de los padres provoca baja autoestima en los hijos. Esta hipótesis tiene el peso de justificar o probar que se dio un cambio

## Tipos de error
- Error tipo 1: ocurre cuando la hipótesis nula es rechazada cuando en realidad es verdadera (riesgo del producto alfa)

-Error de tipo 2: ocurre cuando la hipótesis nula es aceptada cuando en realidad debió ser rechazada (riesgo del consumidor beta)

**P valor** 
El “p value” o p-valor nos muestra la probabilidad de haber obtenido el resultado que hemos obtenido si suponemos que la hipótesis nula es cierta == alfa del error de tipo 1. Controlamos alfa (error de tipo 1), pero para disminuir beta (error e tipo II) debemos aumentar el tamaño muestral.

alfa o p-valor	
p > 0.05	No se prueba diferencia significativa
0.01<p<=0.05	Una diferencia estadística significativa
p<=0.01	Una diferencia estadística altamente significativa





## Regresiones lineales simples

Las regresiones lineales comunmente son usadas para variables dependientes continuas. Aquí la función es lineal, es decir, requiere la determinación de dos parámetros: la pendiente y la ordenada en el origen.

## Aplicación en RStudio

La base de datos con la que se trabajará será "Boston" esta base recoge la mediana del valor de viviendasen 506 áreas residenciales de Boston, teniendo un total de 13 variables, para visualizar esta base de datos una vez instalada la librería se debe escribir en la consola View(Boston), y para obtener más información acerca de la misma, en la ventana de **help** escribir el nombre de la base.

Es necesario instalar el paquete "MASS" que contiene a la librería "Boston"

```{r ins}

library(MASS)
data("Boston")
attach(Boston)

```


Con esta regresión se pretende predecir el valor de la vivienda en función de la tasa impositiva de las propiedades en Boston. Para esto, se emplea la función lm() para generar un modelo lineal.





```{r mod}

reg1<-lm(medv~tax)



```

La función lm() genera un objeto que almacena toda la información del modelo, y para visualizar los principales parámetros del modelo generado se utiliza summary().

```{r sum}

summary(reg1)
```


Se observa que la variable "tax" está significativamnte relacionada con la variable respuesta. La evaluación del modelo en conjunto puede hacerse a partir de los valores RSE o del valor $R^2$ arrojado por el summary.

- Residual standar error (RSE): En promedio, cualquier predicción del modelo se aleja 8.133 unidades del verdadero valor. 
- $R^2$: El predictor **tax**  empleado en el modelo es capaz de explicar el 54.44% de la variabilidad observada en el precio de las viviendas.



Los dos coeficientes de regresión ($β0$ y $β1$) estimados por el modelo son significativos y se pueden interpretar como:

- Intercept($β0$): El valor promedio del precio de la vivienda cuando el tax es 0, es de 32.97 unidades.
- Predictor tax ($β1$): por cada unidad que se incrementa el predictor tax el precio de la vivienda disminuye en promedio 0.025 unidades.

La estimación de todo coeficiente de regresión tiene asociada un error estándar, por lo tanto todo coeficiente de regresión tiene su correspondiente intervalo de confianza.

```{r con}
confint(reg1, level = 0.95)

```

Dado que el p-value del predictor **tax** ha resultado significativo a un 0.05%, su intervalo de confianza del 95% no contiene el valor de 0.

# REGRESION LINEAL MULTIPLE

```{r importar base, message=FALSE, warning=FALSE, include=FALSE}
library(readxl)
PIB <- read_excel("C:/Users/User/Desktop/Bases/PIB.xlsx")
View(PIB)
```


La regresión lineal múltiple permite generar un modelo lineal en el que el valor de la variable dependiente o respuesta (Y) se determina a partir de un conjunto de variables independientes llamadas predictores (X1, X2, X3…). Es una extensión de la regresión lineal simple, por lo que es fundamental comprender esta última. Los modelos de regresión múltiple pueden emplearse para predecir el valor de la variable dependiente o para evaluar la influencia que tienen los predictores sobre ella (esto último se debe que analizar con cautela para no malinterpretar causa-efecto).

Los modelos lineales múltiples siguen la siguiente ecuación:

Yi=(β0+β1X1i+β2X2i+⋯+βnXni)+ei

- β0 : es la ordenada en el origen, el valor de la variable dependiente Y cuando todos los predictores son cero.

- βi: es el efecto promedio que tiene el incremento en una unidad de la variable predictora Xi sobre la variable dependiente Y, manteniéndose constantes el resto de variables. Se conocen como coeficientes parciales de regresión.

- ei: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo.

Es importante tener en cuenta que la magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.

## Modelo

```{r DEFI,include=FALSE}
pib=PIB$PIB
ocupados=PIB$Ocupados
formacion=PIB$FBCF
```


```{r modelo,include=true}
modelo <- lm(pib ~ ocupados + formacion, data = PIB )
summary(modelo)
```

#### Conceptos basicos de la salida de regresion

* Residuals: son las diferencias entre las respuestas observadas en cada combinación de valores de las variables regresoras y la correspondiente predicción de la respuesta calculada.

* Estimate: corresponde a las estimaciones de los coeficientes del modelo. Los coeficientes de las variables regresoras siempre tienen interpretación.

* Std Error: representa el error estándar de cada coeficiente del modelo.

* t value: es el valor que toma el estadístico de prueba.

* Pr(>|t|): corresponde al valor-p para las pruebas de hipótesis de que los coeficientes son ceros. Si esta valor-p es menor que el nivel de significancia, que por defecto es del 5%, entonces se rechaza la hipótesis nula y por lo tanto, existe significancia en la regresión. A menudo, la prueba de hipótesis de que el intecerpto es cero es poco útil, ya que éste depende de la naturaleza del problema. En cambio, las pruebas de hipótesis de que los coeficientes de las variables regresoras son cero, son muy importantes; debido a que, si se prueba que el coeficiente es cero, entonces la variable regresora no tendría ningún aporte al modelo.

* Signif codes: representa el nivel de significancia de las variables regresoras para la predicción del modelo de regresión. Entre más estrellas se le asigne a la regresora, más contribuye con el modelo.

* Residual standard error: es la estimación de la desviación estandar. Este valor da una idea de cuán lejos están los valores del modelo ajustado a los valores observados de la variable respuesta.

* Multiple R-squared: este valor expresa la proporción de la variación explicada por el modelo; es decir, por las variables explicativas. 

* F-statistic: equivale al estadístico F. Éste permite constrastar la hipótesis nula de que el valor poblacional R es cero, lo cual, equivale a contrastar la hipótesis de que los coeficientes de las variables regresoras son cero.

* p-value: indica la importancia del modelo mediante una prueba de hipótesis donde la hipótesis nula es que todos los coeficientes del modelo son cero. 



Con base a lo anterior, el modelo con todas las variables introducidas como predictores tiene un R2 alta (0.977), es capaz de explicar el 97,7% de la variabilidad observada en la esperanza de vida. El p-value del modelo es significativo (2.2e-16) por lo que se puede aceptar que el modelo no es por azar, al menos uno de los coeficientes parciales de regresión es distinto de 0. Logrando observar que todos los coeficientes son significativos y contribuyen al modelo.





## Supuestos del modelo

#### Supuesto de linealidad

Existe linealidad si se presenta una relación significativa entre la variable que se quiere predecir y las otras variables. Puede usarse el coeficiente "R cuadrado ajustado", para saber si existe linealidad (mayor o igual a 0.7 suele ser "indicio" de linealidad).




#### Supuesto de Autocorrelacion

Este supuesto asume que los residuos no están auto-correlacionados, por lo cual son independientes. La autocorrelacion es cuando el residuo en la predicción de un valor es afectado por el residuo en la predicción del valor más cercano. Esta autocorrelacion suele presentarse en series de tiempo.

Para validar la independencia de los residuos puede usarse el test durbin-watson, cuyo resultado estará cerca de 2 cuando los residuos son independientes. Si el valor está entre 1.5 y 2.5 puede concluirse que no existe dependencia de los residuos.

```{r autocorr,include=true}
library(car)
dwt(modelo)
```

De acuerdo con la prueba Durbin Watson, un valor cercano a 2, indica que no hay autocorrelación, > 1 correlaciones positivas; < 1 correlaciones negativas,si esta entre como 1.8 y 2.2 y el valor es menor a 0.01 rechazo la hipotesis nula de que no hay autocorrelacion.




#### Supuesto de homoscedasticidad

Este supuesto asume que los residuos en las predicciones son constantes en cada predicción (es decir, varianza constante). Este supuesto valida que los residuos no aumenta ni disminuye cuando se predicen valores cada vez más altos o mas pequeños. A esta constancia en los errores de predicción le dicen "homocedasticidad", y cuando los errores varían, le dicen "heterocedasticidad".

#### Test de Breush - Pagan
```{r homos,include=true}
library(lmtest)
bptest(modelo)
```
#### Supuesto de normalidad de residuos

Este supuesto asume que los residuos deben presentar una distribución normal, y la ausencia de normalidad supone  poca precisión en los intervalos de confianza creados por el modelo.


#### Test Jarque Bera 

Este test indica que tanto se desvían los indicadores de kurtosis y asimetría de los normales.

```{r norm,include=true}
library(tseries)
jarque.bera.test(x = modelo$residuals)
```

#### Supuesto de multicolinealidad

La multicolinealidad es la relación de dependencia lineal fuerte entre más de dos variables explicativas en una regresión múltiple que incumple el supuesto de Gauss-Markov cuando es exacta.

En otras palabras, la multicolinealidad es la correlación alta entre más de dos variables explicativas.


```{r multicol,include=true}
library(car)
vif(modelo)
```
Si el VIF es mayor o igual a 10, hay problemas de multicolinealidad




