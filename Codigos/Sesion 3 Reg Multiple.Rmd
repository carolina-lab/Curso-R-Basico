---
title: "Sesion 3"
author: "Laboratorio Fintrade"
date: "6/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# REGRESION LINEAL MULTIPLE

```{r importar base, message=FALSE, warning=FALSE, include=FALSE}
library(readxl)
PIB <- read_excel("C:/Users/User/Desktop/Bases/PIB.xlsx")
View(PIB)
```


La regresión lineal múltiple permite generar un modelo lineal en el que el valor de la variable dependiente o respuesta (Y) se determina a partir de un conjunto de variables independientes llamadas predictores (X1, X2, X3…). Es una extensión de la regresión lineal simple, por lo que es fundamental comprender esta última. Los modelos de regresión múltiple pueden emplearse para predecir el valor de la variable dependiente o para evaluar la influencia que tienen los predictores sobre ella (esto último se debe que analizar con cautela para no malinterpretar causa-efecto).

Los modelos lineales múltiples siguen la siguiente ecuación:

Yi=(β0+β1X1i+β2X2i+⋯+βnXni)+ei

- β0 : es la ordenada en el origen, el valor de la variable dependiente Y cuando todos los predictores son cero.

- βi: es el efecto promedio que tiene el incremento en una unidad de la variable predictora Xi sobre la variable dependiente Y, manteniéndose constantes el resto de variables. Se conocen como coeficientes parciales de regresión.

- ei: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo.

Es importante tener en cuenta que la magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.

## Modelo

```{r DEFI,include=FALSE}
pib=PIB$PIB
ocupados=PIB$Ocupados
formacion=PIB$FBCF
```


```{r modelo,include=true}
modelo <- lm(pib ~ ocupados + formacion, data = PIB )
summary(modelo)

```

#### Conceptos basicos de la salida de regresion

* Residuals: son las diferencias entre las respuestas observadas en cada combinación de valores de las variables regresoras y la correspondiente predicción de la respuesta calculada.

* Estimate: corresponde a las estimaciones de los coeficientes del modelo. Los coeficientes de las variables regresoras siempre tienen interpretación.

* Std Error: representa el error estándar de cada coeficiente del modelo.

* t value: es el valor que toma el estadístico de prueba.

* Pr(>|t|): corresponde al valor-p para las pruebas de hipótesis de que los coeficientes son ceros. Si esta valor-p es menor que el nivel de significancia, que por defecto es del 5%, entonces se rechaza la hipótesis nula y por lo tanto, existe significancia en la regresión. A menudo, la prueba de hipótesis de que el intecerpto es cero es poco útil, ya que éste depende de la naturaleza del problema. En cambio, las pruebas de hipótesis de que los coeficientes de las variables regresoras son cero, son muy importantes; debido a que, si se prueba que el coeficiente es cero, entonces la variable regresora no tendría ningún aporte al modelo.

* Signif codes: representa el nivel de significancia de las variables regresoras para la predicción del modelo de regresión. Entre más estrellas se le asigne a la regresora, más contribuye con el modelo.

* Residual standard error: es la estimación de la desviación estandar. Este valor da una idea de cuán lejos están los valores del modelo ajustado a los valores observados de la variable respuesta.

* Multiple R-squared: este valor expresa la proporción de la variación explicada por el modelo; es decir, por las variables explicativas. 

* F-statistic: equivale al estadístico F. Éste permite constrastar la hipótesis nula de que el valor poblacional R es cero, lo cual, equivale a contrastar la hipótesis de que los coeficientes de las variables regresoras son cero.

* p-value: indica la importancia del modelo mediante una prueba de hipótesis donde la hipótesis nula es que todos los coeficientes del modelo son cero. 



Con base a lo anterior, el modelo con todas las variables introducidas como predictores tiene un R2 alta (0.977), es capaz de explicar el 97,7% de la variabilidad observada en la esperanza de vida. El p-value del modelo es significativo (2.2e-16) por lo que se puede aceptar que el modelo no es por azar, al menos uno de los coeficientes parciales de regresión es distinto de 0. Logrando observar que todos los coeficientes son significativos y contribuyen al modelo.





## Supuestos del modelo

#### Supuesto de linealidad

Existe linealidad si se presenta una relación significativa entre la variable que se quiere predecir y las otras variables. Puede usarse el coeficiente "R cuadrado ajustado", para saber si existe linealidad (mayor o igual a 0.7 suele ser "indicio" de linealidad).




#### Supuesto de Autocorrelacion

Este supuesto asume que los residuos no están auto-correlacionados, por lo cual son independientes. La autocorrelacion es cuando el residuo en la predicción de un valor es afectado por el residuo en la predicción del valor más cercano. Esta autocorrelacion suele presentarse en series de tiempo.

Para validar la independencia de los residuos puede usarse el test durbin-watson, cuyo resultado estará cerca de 2 cuando los residuos son independientes. Si el valor está entre 1.5 y 2.5 puede concluirse que no existe dependencia de los residuos.

```{r autocorr,include=true}
library(car)
dwt(modelo)
```

De acuerdo con la prueba Durbin Watson, un valor cercano a 2, indica que no hay autocorrelación, > 1 correlaciones positivas; < 1 correlaciones negativas,si esta entre como 1.8 y 2.2 y el valor es menor a 0.01 rechazo la hipotesis nula de que no hay autocorrelacion.




#### Supuesto de homoscedasticidad

Este supuesto asume que los residuos en las predicciones son constantes en cada predicción (es decir, varianza constante). Este supuesto valida que los residuos no aumenta ni disminuye cuando se predicen valores cada vez más altos o mas pequeños. A esta constancia en los errores de predicción le dicen "homocedasticidad", y cuando los errores varían, le dicen "heterocedasticidad".

#### Test de Breush - Pagan
```{r homos,include=true}
library(lmtest)
bptest(modelo)
```
#### Supuesto de normalidad de residuos

Este supuesto asume que los residuos deben presentar una distribución normal, y la ausencia de normalidad supone  poca precisión en los intervalos de confianza creados por el modelo.


#### Test Jarque Bera 

Este test indica que tanto se desvían los indicadores de kurtosis y asimetría de los normales.

```{r norm,include=true}
library(tseries)
jarque.bera.test(x = modelo$residuals)
```

#### Supuesto de multicolinealidad

La multicolinealidad es la relación de dependencia lineal fuerte entre más de dos variables explicativas en una regresión múltiple que incumple el supuesto de Gauss-Markov cuando es exacta.

En otras palabras, la multicolinealidad es la correlación alta entre más de dos variables explicativas.


```{r multicol,include=true}
library(car)
vif(modelo)


```
Si el VIF es mayor o igual a 10, hay problemas de multicolinealidad








